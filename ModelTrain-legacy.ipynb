{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimador de precios de inmuebles en la ciudad de San Juan, Argentina."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Inicialización: librerías, configuraciones y variables globales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importación de Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install joblib\n",
    "!pip install lightgbm\n",
    "!pip install scikit-learn\n",
    "!pip install nltk\n",
    "!pip install transformers\n",
    "!pip install pandas\n",
    "!pip install spacy\n",
    "!python -m spacy download es_core_news_sm\n",
    "!pip install tensorflow\n",
    "!pip install torch\n",
    "!pip install tf-keras\n",
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sqlite3\n",
    "import re\n",
    "import unicodedata\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import string\n",
    "import joblib\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, RocCurveDisplay, ConfusionMatrixDisplay, roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.collocations import BigramCollocationFinder,BigramAssocMeasures\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('all')\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importación de Base de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buscar la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conn = sqlite3.connect('inmuebles_normalizada.db')\n",
    "df_all = pd.read_sql_query(\"SELECT * FROM inmuebles\", conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_texto(text):\n",
    "    # Convertir a minúsculas\n",
    "    text = text.lower()\n",
    "    # Eliminar signos de puntuación\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Reemplazar tildes\n",
    "    text = text.replace('á', 'a').replace('é', 'e').replace('í', 'i').replace('ó', 'o').replace('ú', 'u').replace('Á', 'A').replace('É', 'E').replace('Í', 'I').replace('Ó', 'O').replace('Ú', 'U')\n",
    "    # Eliminar números y caracteres especiales\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Eliminar espacios en blanco adicionales\n",
    "    text = ' '.join(text.split())\n",
    "    # Tokenizar el texto\n",
    "    tokens = word_tokenize(text, language='spanish')\n",
    "    # Eliminar stop words\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    tokens = [word for word in tokens if not word in stop_words]\n",
    "    # Lematizar las palabras\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    # Unir los tokens de nuevo en una cadena\n",
    "    processed_text = ' '.join(tokens)\n",
    "    return processed_text\n",
    "\n",
    "df_all['comentarios'] = df_all['comentarios'].apply(limpiar_texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizamos un análisis de sentimientos del texto \"comentarios\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cargar el modelo de análisis de sentimientos\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "# Función para analizar el sentimiento de un texto\n",
    "def analyze_sentiment(text):\n",
    "  #Se trunca el texto a un máximo de 512 tokens para que se ajuste al modelo BERT.\n",
    "  result = sentiment_pipeline(text[:512])\n",
    "  return result[0]['label'], result[0]['score']\n",
    "\n",
    "# Aplicar la función al DataFrame\n",
    "df_all[['sentiment_label', 'sentiment_score']] = df_all['comentarios'].apply(lambda x: pd.Series(analyze_sentiment(x)))\n",
    "\n",
    "# Mapear las etiquetas de sentimiento a valores numéricos\n",
    "sentiment_mapping = {'1 star': 1, '2 stars': 2, '3 stars': 3, '4 stars': 4, '5 stars': 5}\n",
    "df_all['des_sentimiento'] = df_all['sentiment_label'].map(sentiment_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizamos una Clasificación de texto de la variable Des"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el pipeline para clasificación de texto con BERT\n",
    "classifier = pipeline('text-classification', model='dccuchile/bert-base-spanish-wwm-uncased')\n",
    "\n",
    "# Función para clasificar el texto\n",
    "def classify_text(text):\n",
    "  result = classifier(text[:775])\n",
    "  return result[0]['label'], result[0]['score']\n",
    "\n",
    "# Aplicar la función al DataFrame\n",
    "df_all[['classification_label', 'classification_score']] = df_all['comentarios'].apply(lambda x: pd.Series(classify_text(x)))\n",
    "df_all['des_clasificacion'] = df_all['classification_label'].map({'LABEL_0': False, 'LABEL_1': True})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar las columnas especificadas\n",
    "df_all = df_all.drop(columns=[\n",
    "    'sentiment_label','sentiment_score', 'classification_score', 'classification_label'\n",
    "])\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame para verificar el resultado\n",
    "df_all.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_backup = df_all.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_backup.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar el vectorizador TF-IDF con un prefijo para evitar duplicados\n",
    "\n",
    "# Vectorización con TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=500, ngram_range=(1, 1), tokenizer=word_tokenize)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df_all['comentarios'])\n",
    "\n",
    "# Obtener los nombres de features con un prefijo para evitar duplicados\n",
    "feature_names = ['tfidf_' + name for name in tfidf_vectorizer.get_feature_names_out()]\n",
    "\n",
    "# Convertir la matriz TF-IDF a un DataFrame de pandas con los nombres prefijados\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "# Concatenar el DataFrame de TF-IDF con el DataFrame original\n",
    "df_all = pd.concat([df_all, tfidf_df], axis=1)\n",
    "\n",
    "# Mostrar las dimensiones del DataFrame\n",
    "df_all.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['precio'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para determinar el mejor tamaño de clase\n",
    "def determinar_mejor_tamanio_clase(df, columna, tamanios_clase):\n",
    "    resultados = []\n",
    "    for tamanio in tamanios_clase:\n",
    "        # Crear clases basadas en el tamaño de clase\n",
    "        bins = list(range(int(df[columna].min()), int(df[columna].max()) + tamanio, tamanio))\n",
    "        df['clase'] = pd.cut(df[columna], bins=bins, include_lowest=True)\n",
    "        \n",
    "        # Calcular la varianza dentro de las clases\n",
    "        varianza_total = df.groupby('clase')[columna].var().sum()\n",
    "        resultados.append((tamanio, varianza_total))\n",
    "    \n",
    "    # Seleccionar el tamaño de clase con la menor varianza\n",
    "    mejor_tamanio = min(resultados, key=lambda x: x[1])\n",
    "    return mejor_tamanio, resultados\n",
    "\n",
    "# Lista de tamaños de clase a evaluar\n",
    "tamanios_clase = range(25000, 50000, 75000)\n",
    "\n",
    "# Determinar el mejor tamaño de clase\n",
    "mejor_tamanio, resultados = determinar_mejor_tamanio_clase(df_all, 'precio', tamanios_clase)\n",
    "\n",
    "# Mostrar el mejor tamaño de clase\n",
    "print(f\"Mejor tamaño de clase: {mejor_tamanio[0]} con varianza total: {mejor_tamanio[1]}\")\n",
    "\n",
    "# Graficar los resultados\n",
    "tamanios, varianzas = zip(*resultados)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(tamanios, varianzas, marker='o', color='blue')\n",
    "plt.title('Evaluación de Tamaños de Clase', fontsize=16)\n",
    "plt.xlabel('Tamaño de Clase', fontsize=12)\n",
    "plt.ylabel('Varianza Total', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar filas con valores nulos en la columna 'precio'\n",
    "df_all = df_all.dropna(subset=['precio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar una nueva columna 'clase' basada en el precio\n",
    "df_all['clase'] = (df_all['precio'] // 25000).astype(int)\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame para verificar el resultado\n",
    "df_all[['precio', 'clase']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check if latitude and longitudee data exist in the dataframe\n",
    "if 'longitude' in df_all.columns and 'latitude' in df_all.columns:\n",
    "    # Create a dataframe with just latitude, longitude and class (price level)\n",
    "    geo_df = df_all[['latitude', 'longitude', 'clase']].copy()\n",
    "    \n",
    "    # Calculate price per square meter if the data is available\n",
    "    if 'superficie_total' in df_all.columns and df_all['superficie_total'].sum() > 0:\n",
    "        # Recover price from class (class * 25000) and calculate price per m²\n",
    "        geo_df['precio_estimado'] = geo_df['clase'] * 25000\n",
    "        geo_df['precio_m2'] = geo_df['precio_estimado'] / df_all['superficie_total'].replace(0, np.nan)\n",
    "        geo_df.dropna(subset=['precio_m2'], inplace=True)\n",
    "    \n",
    "    # Drop rows with missing coordinates\n",
    "    geo_df = geo_df.dropna(subset=['latitude', 'longitude'])\n",
    "    \n",
    "    # Scale the geographic data for clustering\n",
    "    scaler = StandardScaler()\n",
    "    geo_scaled = scaler.fit_transform(geo_df[['latitude', 'longitude']])\n",
    "    \n",
    "    # Determine optimal number of clusters using the elbow method\n",
    "    inertias = []\n",
    "    k_range = range(1, 11)\n",
    "    \n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(geo_scaled)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "    \n",
    "    # Plot elbow curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(k_range, inertias, 'o-')\n",
    "    plt.xlabel('Número de clusters')\n",
    "    plt.ylabel('Inercia')\n",
    "    plt.title('Método del Codo para determinar el número óptimo de clusters')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Choose an appropriate number of clusters based on the elbow curve\n",
    "    optimal_k = 5  # You might want to adjust this after seeing the elbow curve\n",
    "    \n",
    "    # Train the K-means model\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "    geo_df['cluster'] = kmeans.fit_predict(geo_scaled)\n",
    "    \n",
    "    # Calculate average price and price per m² by cluster\n",
    "    cluster_stats = geo_df.groupby('cluster').agg({\n",
    "        'clase': 'mean',\n",
    "        'precio_m2': 'mean' if 'precio_m2' in geo_df.columns else 'count'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Visualize the clusters on a map\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Create a colormap for the clusters\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, optimal_k))\n",
    "    \n",
    "    # Plot each cluster with its corresponding color\n",
    "    for cluster in range(optimal_k):\n",
    "        cluster_points = geo_df[geo_df['cluster'] == cluster]\n",
    "        plt.scatter(\n",
    "            cluster_points['longitude'], \n",
    "            cluster_points['latitude'],\n",
    "            c=[colors[cluster]],\n",
    "            label=f'Cluster {cluster} (Precio Medio: {int(cluster_stats.loc[cluster_stats[\"cluster\"] == cluster, \"clase\"].values[0] * 25000)})',\n",
    "            alpha=0.7\n",
    "        )\n",
    "    \n",
    "    # Add cluster centers\n",
    "    centers = scaler.inverse_transform(kmeans.cluster_centers_)\n",
    "    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.5, marker='X')\n",
    "    \n",
    "    plt.title('Zonas de precios en San Juan basadas en ubicación geográfica')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics by cluster\n",
    "    print(\"\\nEstadísticas por cluster:\")\n",
    "    for i, row in cluster_stats.iterrows():\n",
    "        cluster_num = row['cluster']\n",
    "        avg_price = row['clase'] * 25000\n",
    "        price_metric = row['precio_m2']\n",
    "        \n",
    "        if 'precio_m2' in geo_df.columns:\n",
    "            print(f\"Cluster {cluster_num}: Precio Promedio = ${int(avg_price)}, Precio/m² Promedio = ${int(price_metric)}\")\n",
    "        else:\n",
    "            print(f\"Cluster {cluster_num}: Precio Promedio = ${int(avg_price)}, Cantidad de propiedades: {int(price_metric)}\")\n",
    "else:\n",
    "    print(\"No se encontraron datos de latitude y longitude en el dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "df_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar la variable precio\n",
    "df_all = df_all.drop(columns=['precio'])\n",
    "\n",
    "# Mostrar la forma actual del dataframe\n",
    "print(f\"Forma del dataframe después de eliminar 'precio': {df_all.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Importar las bibliotecas necesarias\n",
    "\n",
    "# Definir características (features) y variable objetivo (target)\n",
    "X = df_all.drop(['clase', 'publicacion', 'image_blob', 'comentarios'], axis=1)\n",
    "y = df_all['clase']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Crear y entrenar el modelo Random Forest\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100, \n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Usar todos los núcleos disponibles\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Redondear predicciones al entero más cercano para obtener las clases\n",
    "y_pred_classes = np.round(y_pred).astype(int)\n",
    "\n",
    "# Evaluar el modelo\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "accuracy = np.mean(y_pred_classes == y_test)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Visualizar la importancia de las características\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "})\n",
    "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Mostrar las 20 características más importantes\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(feature_importance['Feature'][:20], feature_importance['Importance'][:20])\n",
    "plt.xlabel('Importancia')\n",
    "plt.ylabel('Características')\n",
    "plt.title('Top 20 características más importantes (Random Forest)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar la columna 'precio'\n",
    "df_all = df_all.drop(columns=['precio'])\n",
    "\n",
    "# Definir características (features) y variable objetivo (target)\n",
    "X = df_all.drop(['clase', 'publicacion', 'image_blob', 'comentarios'], axis=1)\n",
    "y = df_all['clase']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Crear un dataset de LightGBM\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "# Definir los parámetros para el modelo\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'colsample_bytree': 0.8,\n",
    "    'num_leaves': 31,\n",
    "    'max_depth': 10,\n",
    "    'subsample': 0.8,\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 100\n",
    "}\n",
    "\n",
    "# Entrenar el modelo\n",
    "model = lgb.train(params,\n",
    "                  train_data,\n",
    "                  num_boost_round=100,\n",
    "                  valid_sets=[test_data],\n",
    "                  callbacks=[lgb.early_stopping(stopping_rounds=10)])  # Usar early_stopping callback\n",
    "\n",
    "# Predecir el subconjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Redondear las predicciones al entero más cercano para obtener la clase\n",
    "y_pred_classes = np.round(y_pred).astype(int)\n",
    "\n",
    "# Evaluar el modelo\n",
    "accuracy = np.mean(y_pred_classes == y_test)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la función objetivo para la optimización\n",
    "def objective(trial):\n",
    "    # Definir los hiperparámetros a optimizar\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 50),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 20),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 200)\n",
    "    }\n",
    "\n",
    "    # Entrenar el modelo con los hiperparámetros actuales\n",
    "    model = lgb.train(params, train_data, valid_sets=[test_data])\n",
    "    \n",
    "    # Predecir en el conjunto de prueba\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calcular el RMSE\n",
    "    rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))\n",
    "    return rmse\n",
    "\n",
    "# Crear un estudio para la optimización\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Mostrar los mejores hiperparámetros encontrados\n",
    "print(\"Mejores hiperparámetros:\", study.best_params)\n",
    "\n",
    "# Entrenar el modelo final con los mejores hiperparámetros\n",
    "best_params = study.best_params\n",
    "best_params['objective'] = 'regression'\n",
    "best_params['metric'] = 'rmse'\n",
    "model = lgb.train(best_params, train_data, num_boost_round=100, valid_sets=[test_data], callbacks=[lgb.early_stopping(stopping_rounds=10)])\n",
    "\n",
    "# Predecir y evaluar el modelo optimizado\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))\n",
    "print(f\"RMSE después de la optimización bayesiana: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar los mejores hiperparámetros encontrados por Optuna\n",
    "best_params = study.best_params\n",
    "best_params['objective'] = 'regression'\n",
    "best_params['metric'] = 'rmse'\n",
    "\n",
    "# Crear un nuevo modelo con los mejores hiperparámetros\n",
    "final_model = lgb.train(\n",
    "    best_params,\n",
    "    train_data,\n",
    "    num_boost_round=best_params['n_estimators'],\n",
    "    valid_sets=[test_data],\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=10)]\n",
    ")\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred = final_model.predict(X_test)\n",
    "\n",
    "# Redondear las predicciones al entero más cercano para obtener la clase\n",
    "y_pred_classes = np.round(y_pred).astype(int)\n",
    "\n",
    "# Calcular la precisión (accuracy)\n",
    "accuracy = np.mean(y_pred_classes == y_test)\n",
    "print(f\"Accuracy con los mejores hiperparámetros: {accuracy:.4f}\")\n",
    "\n",
    "# Calcular el RMSE\n",
    "rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))\n",
    "print(f\"RMSE con los mejores hiperparámetros: {rmse:.4f}\")\n",
    "\n",
    "# Mostrar la importancia de las características\n",
    "importance = final_model.feature_importance()\n",
    "feature_names = X_train.columns\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Visualizar las 20 características más importantes\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(importance_df['Feature'][:20], importance_df['Importance'][:20])\n",
    "plt.xlabel('Importancia')\n",
    "plt.ylabel('Características')\n",
    "plt.title('Top 20 características más importantes')\n",
    "plt.gca().invert_yaxis()  # Invertir el eje Y para mostrar la característica más importante arriba\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definir características (features) y variable objetivo (target)\n",
    "X = df_all.drop(['clase', 'publicacion', 'image_blob', 'comentarios'], axis=1)\n",
    "y = df_all['clase']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Crear un dataset de LightGBM\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "# Definir los parámetros para el modelo\n",
    "params = best_params.copy()\n",
    "# Entrenar el modelo\n",
    "model = lgb.train(params,\n",
    "                  train_data,\n",
    "                  num_boost_round=100,\n",
    "                  valid_sets=[test_data],\n",
    "                  callbacks=[lgb.early_stopping(stopping_rounds=10)])  # Usar early_stopping callback\n",
    "\n",
    "# Predecir el subconjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Redondear las predicciones al entero más cercano para obtener la clase\n",
    "y_pred_classes = np.round(y_pred).astype(int)\n",
    "\n",
    "# Evaluar el modelo\n",
    "accuracy = np.mean(y_pred_classes == y_test)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
