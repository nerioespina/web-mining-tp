{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimador de precios de inmuebles en la ciudad de San Juan, Argentina."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Inicialización: librerías, configuraciones y variables globales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importación de Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install joblib\n",
    "# !pip install lightgbm\n",
    "# !pip install scikit-learn\n",
    "# !pip install nltk\n",
    "# !pip install transformers\n",
    "# !pip install pandas\n",
    "# !pip install spacy\n",
    "# !python -m spacy download es_core_news_sm\n",
    "# !pip install tensorflow\n",
    "# !pip install torch\n",
    "# !pip install tf-keras\n",
    "# !pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import sqlite3\n",
    "import re\n",
    "import unicodedata\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import string\n",
    "import joblib\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, RocCurveDisplay, ConfusionMatrixDisplay, roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importación de Base de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buscar la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conn = sqlite3.connect('inmuebles_final.db')\n",
    "df_all = pd.read_sql_query(\"SELECT * FROM vista_inmuebles\", conn)\n",
    "conn.close()\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all.dropna(subset=['precio'])\n",
    "df_all['precio'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para determinar el mejor tamaño de clase\n",
    "def determinar_mejor_tamanio_clase(df, columna, tamanios_clase):\n",
    "    resultados = []\n",
    "    for tamanio in tamanios_clase:\n",
    "        # Crear clases basadas en el tamaño de clase\n",
    "        bins = list(range(int(df[columna].min()), int(df[columna].max()) + tamanio, tamanio))\n",
    "        df['clase'] = pd.cut(df[columna], bins=bins, include_lowest=True)\n",
    "        \n",
    "        # Calcular la varianza dentro de las clases\n",
    "        varianza_total = df.groupby('clase')[columna].var().sum()\n",
    "        resultados.append((tamanio, varianza_total))\n",
    "    \n",
    "    # Seleccionar el tamaño de clase con la menor varianza\n",
    "    mejor_tamanio = min(resultados, key=lambda x: x[1])\n",
    "    return mejor_tamanio, resultados\n",
    "\n",
    "# Lista de tamaños de clase a evaluar\n",
    "tamanios_clase = range(25000, 50000, 75000)\n",
    "\n",
    "# Determinar el mejor tamaño de clase\n",
    "mejor_tamanio, resultados = determinar_mejor_tamanio_clase(df_all, 'precio', tamanios_clase)\n",
    "\n",
    "# Mostrar el mejor tamaño de clase\n",
    "print(f\"Mejor tamaño de clase: {mejor_tamanio[0]} con varianza total: {mejor_tamanio[1]}\")\n",
    "\n",
    "# Graficar los resultados\n",
    "tamanios, varianzas = zip(*resultados)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(tamanios, varianzas, marker='o', color='blue')\n",
    "plt.title('Evaluación de Tamaños de Clase', fontsize=16)\n",
    "plt.xlabel('Tamaño de Clase', fontsize=12)\n",
    "plt.ylabel('Varianza Total', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar filas con valores nulos en la columna 'precio'\n",
    "df_all = df_all.dropna(subset=['precio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar una nueva columna 'clase' basada en el precio\n",
    "df_all['clase'] = (df_all['precio'] // 25000).astype(int)\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame para verificar el resultado\n",
    "df_all[['precio', 'clase']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Agregar columnas de precio por metro cuadrado cubierto y total\n",
    "df_all['precio_m2_cubierta'] = df_all['precio'] / df_all['superficie_cubierta'].replace(0, np.nan)\n",
    "df_all['precio_m2_total'] = df_all['precio'] / df_all['superficie_total'].replace(0, np.nan)\n",
    "\n",
    "# Mostrar las primeras filas para verificar\n",
    "df_all[['precio', 'superficie_cubierta', 'precio_m2_cubierta', 'superficie_total', 'precio_m2_total']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame con las columnas necesarias\n",
    "df_boxplot_m2 = df_all[['precio_m2_cubierta', 'precio_m2_total']].copy()\n",
    "\n",
    "# Renombrar columnas para facilitar la visualización\n",
    "df_boxplot_m2 = df_boxplot_m2.rename(columns={\n",
    "    'precio_m2_cubierta': 'Precio por m² Cubierta',\n",
    "    'precio_m2_total': 'Precio por m² Total'\n",
    "})\n",
    "\n",
    "# Transformar el DataFrame a formato largo para seaborn\n",
    "df_melted_m2 = df_boxplot_m2.melt(var_name='Tipo de Precio', value_name='Valor')\n",
    "\n",
    "# Crear el boxplot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=df_melted_m2, x='Tipo de Precio', y='Valor')\n",
    "plt.title('Distribución de Precios por Metro Cuadrado', fontsize=16)\n",
    "plt.xlabel('Tipo de Precio', fontsize=12)\n",
    "plt.ylabel('Valor', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar registros con precio por metro cuadrado cubierto o total mayor a 10000\n",
    "df_precio_alto = df_all[(df_all['precio_m2_cubierta'] > 10000) | (df_all['precio_m2_total'] > 10000)]\n",
    "\n",
    "# Imprimir los registros filtrados\n",
    "print(df_precio_alto[['precio', 'superficie_cubierta', 'superficie_total']].head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Verificar si los campos latitude y longitude existen en el dataframe\n",
    "if 'longitude' in df_all.columns and 'latitude' in df_all.columns:\n",
    "    # Crear un dataframe con latitude, longitude y precio\n",
    "    geo_df = df_all[['latitude', 'longitude', 'precio', 'superficie_cubierta', 'superficie_total']].copy()\n",
    "    \n",
    "    # Eliminar filas con valores nulos en latitude, longitude o precio\n",
    "    geo_df = geo_df.dropna(subset=['latitude', 'longitude', 'precio'])\n",
    "    \n",
    "    # Escalar los datos geográficos para el clustering\n",
    "    scaler = StandardScaler()\n",
    "    geo_scaled = scaler.fit_transform(geo_df[['latitude', 'longitude']])\n",
    "    \n",
    "    # Determinar el número óptimo de clusters usando el método del codo\n",
    "    inertias = []\n",
    "    k_range = range(1, 11)\n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(geo_scaled)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "    \n",
    "    # Graficar el método del codo\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(k_range, inertias, 'o-')\n",
    "    plt.xlabel('Número de clusters')\n",
    "    plt.ylabel('Inercia')\n",
    "    plt.title('Método del Codo para determinar el número óptimo de clusters')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Elegir un número óptimo de clusters basado en el gráfico del codo\n",
    "    optimal_k = 5  # Ajustar según el gráfico del codo\n",
    "    \n",
    "    # Entrenar el modelo K-means\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "    geo_df['cluster'] = kmeans.fit_predict(geo_scaled)\n",
    "    \n",
    "    # Calcular el precio por metro cuadrado de superficie cubierta y total\n",
    "    geo_df['precio_m2_cubierta'] = geo_df['precio'] / geo_df['superficie_cubierta'].replace(0, np.nan)\n",
    "    geo_df['precio_m2_total'] = geo_df['precio'] / geo_df['superficie_total'].replace(0, np.nan)\n",
    "    \n",
    "    # Agregar los valores calculados al dataframe original\n",
    "    df_all = df_all.merge(geo_df[['latitude', 'longitude', 'precio_m2_cubierta', 'precio_m2_total']], \n",
    "                          on=['latitude', 'longitude'], how='left')\n",
    "    \n",
    "    # Mostrar las primeras filas para verificar\n",
    "    print(df_all[['latitude', 'longitude', 'precio_m2_cubierta', 'precio_m2_total']].head())\n",
    "else:\n",
    "    print(\"No se encontraron los campos latitude y longitude en el dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[['clase','latitude', 'longitude', 'precio_m2_cubierta', 'precio_m2_total']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar las nuevas columnas multiplicando el precio por las superficies correspondientes\n",
    "df_all['precio_estimado_superficie_total'] = df_all['precio_m2_total'] * df_all['superficie_total']\n",
    "df_all['precio_estimado_superficie_cubierta'] = df_all['precio_m2_cubierta'] * df_all['superficie_cubierta']\n",
    "\n",
    "# Mostrar las primeras filas para verificar\n",
    "df_all[['precio', 'superficie_total', 'precio_estimado_superficie_total', 'superficie_cubierta', 'precio_estimado_superficie_cubierta']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['clase_estimada_superficie_cubierta'] = (df_all['precio_estimado_superficie_cubierta'] // 25000)\n",
    "df_all['clase_estimada_superficie_total'] = (df_all['precio_estimado_superficie_total'] // 25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[['clase', 'clase_estimada_superficie_cubierta', 'clase_estimada_superficie_total', 'precio', 'superficie_total', 'precio_estimado_superficie_total', 'superficie_cubierta', 'precio_estimado_superficie_cubierta']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Crear un DataFrame para las clases reales y estimadas\n",
    "df_boxplot = df_all[['clase', 'clase_estimada_superficie_cubierta', 'clase_estimada_superficie_total']].copy()\n",
    "\n",
    "# Renombrar columnas para facilitar la visualización\n",
    "df_boxplot = df_boxplot.rename(columns={\n",
    "    'clase': 'Clase Real',\n",
    "    'clase_estimada_superficie_cubierta': 'Clase Estimada (Superficie Cubierta)',\n",
    "    'clase_estimada_superficie_total': 'Clase Estimada (Superficie Total)'\n",
    "})\n",
    "\n",
    "# Transformar el DataFrame a formato largo para seaborn\n",
    "df_melted = df_boxplot.melt(var_name='Tipo de Clase', value_name='Valor')\n",
    "\n",
    "# Crear el boxplot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=df_melted, x='Tipo de Clase', y='Valor')\n",
    "plt.title('Comparación de Clases Reales y Estimadas', fontsize=16)\n",
    "plt.xlabel('Tipo de Clase', fontsize=12)\n",
    "plt.ylabel('Valor de la Clase', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar filas con valores nulos en 'clase' o 'clase_estimada_superficie_cubierta'\n",
    "df_valid = df_all.dropna(subset=['clase', 'clase_estimada_superficie_cubierta'])\n",
    "\n",
    "# Calcular el número de coincidencias exactas\n",
    "coincidencias = (df_valid['clase'] == df_valid['clase_estimada_superficie_cubierta']).sum()\n",
    "\n",
    "# Calcular el total de filas válidas\n",
    "total_filas_validas = len(df_valid)\n",
    "\n",
    "# Calcular el accuracy\n",
    "accuracy = coincidencias / total_filas_validas\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar filas con valores nulos en 'clase' o 'clase_estimada_superficie_total'\n",
    "df_valid = df_all.dropna(subset=['clase', 'clase_estimada_superficie_total'])\n",
    "\n",
    "# Calcular el número de coincidencias exactas\n",
    "coincidencias = (df_valid['clase'] == df_valid['clase_estimada_superficie_total']).sum()\n",
    "\n",
    "# Calcular el total de filas válidas\n",
    "total_filas_validas = len(df_valid)\n",
    "\n",
    "# Calcular el accuracy\n",
    "accuracy = coincidencias / total_filas_validas\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check if latitude and longitudee data exist in the dataframe\n",
    "if 'longitude' in df_all.columns and 'latitude' in df_all.columns:\n",
    "    # Create a dataframe with just latitude, longitude and class (price level)\n",
    "    geo_df = df_all[['latitude', 'longitude', 'clase']].copy()\n",
    "    \n",
    "    # Calculate price per square meter if the data is available\n",
    "    if 'superficie_total' in df_all.columns and df_all['superficie_total'].sum() > 0:\n",
    "        # Recover price from class (class * 25000) and calculate price per m²\n",
    "        geo_df['precio_estimado'] = geo_df['clase'] * 25000\n",
    "        geo_df['precio_m2'] = geo_df['precio_estimado'] / df_all['superficie_total'].replace(0, np.nan)\n",
    "        geo_df.dropna(subset=['precio_m2'], inplace=True)\n",
    "    \n",
    "    # Drop rows with missing coordinates\n",
    "    geo_df = geo_df.dropna(subset=['latitude', 'longitude'])\n",
    "    \n",
    "    # Scale the geographic data for clustering\n",
    "    scaler = StandardScaler()\n",
    "    geo_scaled = scaler.fit_transform(geo_df[['latitude', 'longitude']])\n",
    "    \n",
    "    # Determine optimal number of clusters using the elbow method\n",
    "    inertias = []\n",
    "    k_range = range(1, 11)\n",
    "    \n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(geo_scaled)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "    \n",
    "    # Plot elbow curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(k_range, inertias, 'o-')\n",
    "    plt.xlabel('Número de clusters')\n",
    "    plt.ylabel('Inercia')\n",
    "    plt.title('Método del Codo para determinar el número óptimo de clusters')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Choose an appropriate number of clusters based on the elbow curve\n",
    "    optimal_k = 5  # You might want to adjust this after seeing the elbow curve\n",
    "    \n",
    "    # Train the K-means model\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "    geo_df['cluster'] = kmeans.fit_predict(geo_scaled)\n",
    "    \n",
    "    # Calculate average price and price per m² by cluster\n",
    "    cluster_stats = geo_df.groupby('cluster').agg({\n",
    "        'clase': 'mean',\n",
    "        'precio_m2': 'mean' if 'precio_m2' in geo_df.columns else 'count'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Visualize the clusters on a map\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Create a colormap for the clusters\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, optimal_k))\n",
    "    \n",
    "    # Plot each cluster with its corresponding color\n",
    "    for cluster in range(optimal_k):\n",
    "        cluster_points = geo_df[geo_df['cluster'] == cluster]\n",
    "        plt.scatter(\n",
    "            cluster_points['longitude'], \n",
    "            cluster_points['latitude'],\n",
    "            c=[colors[cluster]],\n",
    "            label=f'Cluster {cluster} (Precio Medio: {int(cluster_stats.loc[cluster_stats[\"cluster\"] == cluster, \"clase\"].values[0] * 25000)})',\n",
    "            alpha=0.7\n",
    "        )\n",
    "    \n",
    "    # Add cluster centers\n",
    "    centers = scaler.inverse_transform(kmeans.cluster_centers_)\n",
    "    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.5, marker='X')\n",
    "    \n",
    "    plt.title('Zonas de precios en San Juan basadas en ubicación geográfica')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics by cluster\n",
    "    print(\"\\nEstadísticas por cluster:\")\n",
    "    for i, row in cluster_stats.iterrows():\n",
    "        cluster_num = row['cluster']\n",
    "        avg_price = row['clase'] * 25000\n",
    "        price_metric = row['precio_m2']\n",
    "        \n",
    "        if 'precio_m2' in geo_df.columns:\n",
    "            print(f\"Cluster {cluster_num}: Precio Promedio = ${int(avg_price)}, Precio/m² Promedio = ${int(price_metric)}\")\n",
    "        else:\n",
    "            print(f\"Cluster {cluster_num}: Precio Promedio = ${int(avg_price)}, Cantidad de propiedades: {int(price_metric)}\")\n",
    "else:\n",
    "    print(\"No se encontraron datos de latitude y longitude en el dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check if latitude and longitudee data exist in the dataframe\n",
    "if 'longitude' in df_all.columns and 'latitude' in df_all.columns:\n",
    "    # Create a dataframe with just latitude, longitude and class (price level)\n",
    "    geo_df = df_all[['latitude', 'longitude', 'clase']].copy()\n",
    "    \n",
    "    # Calculate price per square meter if the data is available\n",
    "    if 'superficie_cubierta' in df_all.columns and df_all['superficie_cubierta'].sum() > 0:\n",
    "        # Recover price from class (class * 25000) and calculate price per m²\n",
    "        geo_df['precio_estimado'] = geo_df['clase'] * 25000\n",
    "        geo_df['precio_m2'] = geo_df['precio_estimado'] / df_all['superficie_cubierta'].replace(0, np.nan)\n",
    "        geo_df.dropna(subset=['precio_m2'], inplace=True)\n",
    "    \n",
    "    # Drop rows with missing coordinates\n",
    "    geo_df = geo_df.dropna(subset=['latitude', 'longitude'])\n",
    "    \n",
    "    # Scale the geographic data for clustering\n",
    "    scaler = StandardScaler()\n",
    "    geo_scaled = scaler.fit_transform(geo_df[['latitude', 'longitude']])\n",
    "    \n",
    "    # Determine optimal number of clusters using the elbow method\n",
    "    inertias = []\n",
    "    k_range = range(1, 11)\n",
    "    \n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(geo_scaled)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "    \n",
    "    # Plot elbow curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(k_range, inertias, 'o-')\n",
    "    plt.xlabel('Número de clusters')\n",
    "    plt.ylabel('Inercia')\n",
    "    plt.title('Método del Codo para determinar el número óptimo de clusters')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Choose an appropriate number of clusters based on the elbow curve\n",
    "    optimal_k = 5  # You might want to adjust this after seeing the elbow curve\n",
    "    \n",
    "    # Train the K-means model\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "    geo_df['cluster'] = kmeans.fit_predict(geo_scaled)\n",
    "    \n",
    "    # Calculate average price and price per m² by cluster\n",
    "    cluster_stats = geo_df.groupby('cluster').agg({\n",
    "        'clase': 'mean',\n",
    "        'precio_m2': 'mean' if 'precio_m2' in geo_df.columns else 'count'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Visualize the clusters on a map\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Create a colormap for the clusters\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, optimal_k))\n",
    "    \n",
    "    # Plot each cluster with its corresponding color\n",
    "    for cluster in range(optimal_k):\n",
    "        cluster_points = geo_df[geo_df['cluster'] == cluster]\n",
    "        plt.scatter(\n",
    "            cluster_points['longitude'], \n",
    "            cluster_points['latitude'],\n",
    "            c=[colors[cluster]],\n",
    "            label=f'Cluster {cluster} (Precio Medio: {int(cluster_stats.loc[cluster_stats[\"cluster\"] == cluster, \"clase\"].values[0] * 25000)})',\n",
    "            alpha=0.7\n",
    "        )\n",
    "    \n",
    "    # Add cluster centers\n",
    "    centers = scaler.inverse_transform(kmeans.cluster_centers_)\n",
    "    plt.scatter(centers[:, 1], centers[:, 0], c='black', s=200, alpha=0.5, marker='X')\n",
    "    \n",
    "    plt.title('Zonas de precios en San Juan basadas en ubicación geográfica')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics by cluster\n",
    "    print(\"\\nEstadísticas por cluster:\")\n",
    "    for i, row in cluster_stats.iterrows():\n",
    "        cluster_num = row['cluster']\n",
    "        avg_price = row['clase'] * 25000\n",
    "        price_metric = row['precio_m2']\n",
    "        \n",
    "        if 'precio_m2' in geo_df.columns:\n",
    "            print(f\"Cluster {cluster_num}: Precio Promedio = ${int(avg_price)}, Precio/m² Promedio = ${int(price_metric)}\")\n",
    "        else:\n",
    "            print(f\"Cluster {cluster_num}: Precio Promedio = ${int(avg_price)}, Cantidad de propiedades: {int(price_metric)}\")\n",
    "else:\n",
    "    print(\"No se encontraron datos de latitude y longitude en el dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Agregar columna de precio estimado por metro cuadrado de superficie cubierta\n",
    "if 'superficie_cubierta' in df_all.columns and 'cluster_precio_estimado' in df_all.columns:\n",
    "    # Calcular el precio estimado por metro cuadrado\n",
    "    df_all['precio_m2_estimado'] = df_all['cluster_precio_estimado'] * 25000 / df_all['superficie_cubierta'].replace(0, np.nan)\n",
    "    \n",
    "    # Multiplicar el precio por metro cuadrado por los metros cuadrados de superficie cubierta\n",
    "    df_all['precio_estimado'] = df_all['precio_m2_estimado'] * df_all['superficie_cubierta']\n",
    "    \n",
    "    # Mostrar las primeras filas para verificar\n",
    "    print(df_all[['superficie_cubierta', 'cluster_precio_estimado', 'precio_m2_estimado', 'precio_estimado']].head())\n",
    "else:\n",
    "    print(\"No se encontraron las columnas necesarias para realizar el cálculo.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "df_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "df_all[['precio', 'clase', 'cluster_zona', 'cluster_precio_estimado', 'precio_estimado']].head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar la variable precio\n",
    "df_all = df_all.drop(columns=['precio'])\n",
    "\n",
    "# Mostrar la forma actual del dataframe\n",
    "print(f\"Forma del dataframe después de eliminar 'precio': {df_all.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conn = sqlite3.connect('inmuebles_final.db')\n",
    "df_all = pd.read_sql_query(\"SELECT * FROM vista_inmuebles where categoria_id = 4\", conn)\n",
    "conn.close()\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_all.select_dtypes(exclude=['object'])\n",
    "df_all.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminar registros con valores nulos en ambas columnas\n",
    "df_all = df_all.dropna(subset=['superficie_total', 'superficie_cubierta'])\n",
    "\n",
    "# Mostrar las primeras filas para verificar\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la matriz de correlación\n",
    "correlation_matrix = df_all.corr()\n",
    "\n",
    "# Visualizar la matriz de correlación\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True)\n",
    "plt.title(\"Matriz de Correlación\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Eliminar las columnas especificadas del DataFrame\n",
    "df_all = df_all.drop(columns=['visitas', 'anuncio_id', 'banos_escala', 'apto_credito'])\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame actualizado para verificar\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Importar las bibliotecas necesarias\n",
    "\n",
    "# Definir características (features) y variable objetivo (target)\n",
    "X = df_all[['superficie_cubierta', 'en_barrio_privado', 'banos_escala', 'dormitorios_escala', 'garages_escala']]\n",
    "y = df_all['clase']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Crear y entrenar el modelo Random Forest\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100, \n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Usar todos los núcleos disponibles\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Redondear predicciones al entero más cercano para obtener las clases\n",
    "y_pred_classes = np.round(y_pred).astype(int)\n",
    "\n",
    "# Evaluar el modelo\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "accuracy = np.mean(y_pred_classes == y_test)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse:.4f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Visualizar la importancia de las características\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "})\n",
    "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Mostrar las 20 características más importantes\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(feature_importance['Feature'][:20], feature_importance['Importance'][:20])\n",
    "plt.xlabel('Importancia')\n",
    "plt.ylabel('Características')\n",
    "plt.title('Top 20 características más importantes (Random Forest)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar la columna 'precio'\n",
    "# df_all = df_all.drop(columns=['precio'])\n",
    "\n",
    "# Definir características (features) y variable objetivo (target)\n",
    "X = df_all[['superficie_cubierta', 'en_barrio_privado', 'banos_escala', 'dormitorios_escala', 'garages_escala']]\n",
    "y = df_all['clase']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Crear un dataset de LightGBM\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "# Definir los parámetros para el modelo\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'colsample_bytree': 0.8,\n",
    "    'num_leaves': 31,\n",
    "    'max_depth': 10,\n",
    "    'subsample': 0.8,\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 100\n",
    "}\n",
    "\n",
    "# Entrenar el modelo\n",
    "model = lgb.train(params,\n",
    "                  train_data,\n",
    "                  num_boost_round=100,\n",
    "                  valid_sets=[test_data],\n",
    "                  callbacks=[lgb.early_stopping(stopping_rounds=10)])  # Usar early_stopping callback\n",
    "\n",
    "# Predecir el subconjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Redondear las predicciones al entero más cercano para obtener la clase\n",
    "y_pred_classes = np.round(y_pred).astype(int)\n",
    "\n",
    "# Evaluar el modelo\n",
    "accuracy = np.mean(y_pred_classes == y_test)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la función objetivo para la optimización\n",
    "def objective(trial):\n",
    "    # Definir los hiperparámetros a optimizar\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 50),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 20),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 200)\n",
    "    }\n",
    "\n",
    "    # Entrenar el modelo con los hiperparámetros actuales\n",
    "    model = lgb.train(params, train_data, valid_sets=[test_data])\n",
    "    \n",
    "    # Predecir en el conjunto de prueba\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calcular el RMSE\n",
    "    rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))\n",
    "    return rmse\n",
    "\n",
    "# Crear un estudio para la optimización\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Mostrar los mejores hiperparámetros encontrados\n",
    "print(\"Mejores hiperparámetros:\", study.best_params)\n",
    "\n",
    "# Entrenar el modelo final con los mejores hiperparámetros\n",
    "best_params = study.best_params\n",
    "best_params['objective'] = 'regression'\n",
    "best_params['metric'] = 'rmse'\n",
    "model = lgb.train(best_params, train_data, num_boost_round=100, valid_sets=[test_data], callbacks=[lgb.early_stopping(stopping_rounds=10)])\n",
    "\n",
    "# Predecir y evaluar el modelo optimizado\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))\n",
    "print(f\"RMSE después de la optimización bayesiana: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar los mejores hiperparámetros encontrados por Optuna\n",
    "best_params = study.best_params\n",
    "best_params['objective'] = 'regression'\n",
    "best_params['metric'] = 'rmse'\n",
    "\n",
    "# Crear un nuevo modelo con los mejores hiperparámetros\n",
    "final_model = lgb.train(\n",
    "    best_params,\n",
    "    train_data,\n",
    "    num_boost_round=best_params['n_estimators'],\n",
    "    valid_sets=[test_data],\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=10)]\n",
    ")\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred = final_model.predict(X_test)\n",
    "\n",
    "# Redondear las predicciones al entero más cercano para obtener la clase\n",
    "y_pred_classes = np.round(y_pred).astype(int)\n",
    "\n",
    "# Calcular la precisión (accuracy)\n",
    "accuracy = np.mean(y_pred_classes == y_test)\n",
    "print(f\"Accuracy con los mejores hiperparámetros: {accuracy:.4f}\")\n",
    "\n",
    "# Calcular el RMSE\n",
    "rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))\n",
    "print(f\"RMSE con los mejores hiperparámetros: {rmse:.4f}\")\n",
    "\n",
    "# Mostrar la importancia de las características\n",
    "importance = final_model.feature_importance()\n",
    "feature_names = X_train.columns\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Visualizar las 20 características más importantes\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(importance_df['Feature'][:20], importance_df['Importance'][:20])\n",
    "plt.xlabel('Importancia')\n",
    "plt.ylabel('Características')\n",
    "plt.title('Top 20 características más importantes')\n",
    "plt.gca().invert_yaxis()  # Invertir el eje Y para mostrar la característica más importante arriba\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definir características (features) y variable objetivo (target)\n",
    "X = df_all.drop(['clase', 'publicacion', 'image_blob', 'comentarios'], axis=1)\n",
    "y = df_all['clase']\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Crear un dataset de LightGBM\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "# Definir los parámetros para el modelo\n",
    "params = best_params.copy()\n",
    "# Entrenar el modelo\n",
    "model = lgb.train(params,\n",
    "                  train_data,\n",
    "                  num_boost_round=100,\n",
    "                  valid_sets=[test_data],\n",
    "                  callbacks=[lgb.early_stopping(stopping_rounds=10)])  # Usar early_stopping callback\n",
    "\n",
    "# Predecir el subconjunto de prueba\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Redondear las predicciones al entero más cercano para obtener la clase\n",
    "y_pred_classes = np.round(y_pred).astype(int)\n",
    "\n",
    "# Evaluar el modelo\n",
    "accuracy = np.mean(y_pred_classes == y_test)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
